{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aeddd68-f722-43fc-a54f-dfe7e4d3409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# conda environments:\n",
      "#\n",
      "base                   C:\\anaconda3\n",
      "pyspark_env            C:\\anaconda3\\envs\\pyspark_env\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f65f344-ccbc-4fea-aff3-81d86f97cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate pyspark_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea5de8a2-2427-4f84-818a-b9a31970a9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-4.0.0.tar.gz (434.1 MB)\n",
      "     ---------------------------------------- 0.0/434.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 2.1/434.1 MB 9.8 MB/s eta 0:00:45\n",
      "     ---------------------------------------- 3.9/434.1 MB 9.0 MB/s eta 0:00:48\n",
      "      --------------------------------------- 5.8/434.1 MB 8.8 MB/s eta 0:00:49\n",
      "      --------------------------------------- 7.3/434.1 MB 8.9 MB/s eta 0:00:49\n",
      "      --------------------------------------- 8.7/434.1 MB 8.1 MB/s eta 0:00:53\n",
      "      --------------------------------------- 9.4/434.1 MB 7.3 MB/s eta 0:00:58\n",
      "      -------------------------------------- 10.7/434.1 MB 7.0 MB/s eta 0:01:01\n",
      "     - ------------------------------------- 11.5/434.1 MB 6.8 MB/s eta 0:01:03\n",
      "     - ------------------------------------- 12.8/434.1 MB 6.5 MB/s eta 0:01:05\n",
      "     - ------------------------------------- 13.6/434.1 MB 6.3 MB/s eta 0:01:07\n",
      "     - ------------------------------------- 15.2/434.1 MB 6.3 MB/s eta 0:01:07\n",
      "     - ------------------------------------- 17.0/434.1 MB 6.5 MB/s eta 0:01:04\n",
      "     - ------------------------------------- 18.6/434.1 MB 6.6 MB/s eta 0:01:04\n",
      "     - ------------------------------------- 20.4/434.1 MB 6.8 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 22.5/434.1 MB 6.9 MB/s eta 0:01:00\n",
      "     -- ------------------------------------ 24.4/434.1 MB 7.0 MB/s eta 0:00:59\n",
      "     -- ------------------------------------ 26.5/434.1 MB 7.2 MB/s eta 0:00:57\n",
      "     -- ------------------------------------ 28.6/434.1 MB 7.3 MB/s eta 0:00:56\n",
      "     -- ------------------------------------ 30.4/434.1 MB 7.4 MB/s eta 0:00:55\n",
      "     -- ------------------------------------ 32.5/434.1 MB 7.5 MB/s eta 0:00:54\n",
      "     --- ----------------------------------- 34.3/434.1 MB 7.5 MB/s eta 0:00:53\n",
      "     --- ----------------------------------- 36.4/434.1 MB 7.6 MB/s eta 0:00:53\n",
      "     --- ----------------------------------- 38.5/434.1 MB 7.7 MB/s eta 0:00:52\n",
      "     --- ----------------------------------- 40.4/434.1 MB 7.8 MB/s eta 0:00:51\n",
      "     --- ----------------------------------- 42.5/434.1 MB 7.8 MB/s eta 0:00:51\n",
      "     --- ----------------------------------- 44.3/434.1 MB 7.9 MB/s eta 0:00:50\n",
      "     ---- ---------------------------------- 46.4/434.1 MB 7.9 MB/s eta 0:00:49\n",
      "     ---- ---------------------------------- 48.2/434.1 MB 8.0 MB/s eta 0:00:49\n",
      "     ---- ---------------------------------- 50.3/434.1 MB 8.0 MB/s eta 0:00:49\n",
      "     ---- ---------------------------------- 52.2/434.1 MB 8.0 MB/s eta 0:00:48\n",
      "     ---- ---------------------------------- 53.7/434.1 MB 8.0 MB/s eta 0:00:48\n",
      "     ----- --------------------------------- 55.8/434.1 MB 8.0 MB/s eta 0:00:48\n",
      "     ----- --------------------------------- 57.7/434.1 MB 8.1 MB/s eta 0:00:47\n",
      "     ----- --------------------------------- 59.0/434.1 MB 8.0 MB/s eta 0:00:47\n",
      "     ----- --------------------------------- 60.0/434.1 MB 7.9 MB/s eta 0:00:48\n",
      "     ----- --------------------------------- 61.3/434.1 MB 7.9 MB/s eta 0:00:48\n",
      "     ----- --------------------------------- 62.7/434.1 MB 7.8 MB/s eta 0:00:48\n",
      "     ----- --------------------------------- 64.0/434.1 MB 7.8 MB/s eta 0:00:48\n",
      "     ----- --------------------------------- 65.3/434.1 MB 7.7 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 67.1/434.1 MB 7.7 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 68.9/434.1 MB 7.8 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 70.8/434.1 MB 7.8 MB/s eta 0:00:47\n",
      "     ------ -------------------------------- 72.9/434.1 MB 7.8 MB/s eta 0:00:47\n",
      "     ------ -------------------------------- 74.7/434.1 MB 7.8 MB/s eta 0:00:46\n",
      "     ------ -------------------------------- 76.8/434.1 MB 7.9 MB/s eta 0:00:46\n",
      "     ------- ------------------------------- 78.9/434.1 MB 7.9 MB/s eta 0:00:45\n",
      "     ------- ------------------------------- 80.7/434.1 MB 7.9 MB/s eta 0:00:45\n",
      "     ------- ------------------------------- 82.8/434.1 MB 8.0 MB/s eta 0:00:45\n",
      "     ------- ------------------------------- 84.7/434.1 MB 8.0 MB/s eta 0:00:44\n",
      "     ------- ------------------------------- 86.0/434.1 MB 7.9 MB/s eta 0:00:44\n",
      "     ------- ------------------------------- 87.8/434.1 MB 8.0 MB/s eta 0:00:44\n",
      "     -------- ------------------------------ 89.9/434.1 MB 8.0 MB/s eta 0:00:44\n",
      "     -------- ------------------------------ 91.8/434.1 MB 8.0 MB/s eta 0:00:43\n",
      "     -------- ------------------------------ 93.8/434.1 MB 8.0 MB/s eta 0:00:43\n",
      "     -------- ------------------------------ 95.9/434.1 MB 8.0 MB/s eta 0:00:43\n",
      "     -------- ------------------------------ 97.8/434.1 MB 8.1 MB/s eta 0:00:42\n",
      "     -------- ------------------------------ 99.9/434.1 MB 8.1 MB/s eta 0:00:42\n",
      "     -------- ----------------------------- 101.7/434.1 MB 8.1 MB/s eta 0:00:42\n",
      "     --------- ---------------------------- 103.5/434.1 MB 8.1 MB/s eta 0:00:41\n",
      "     --------- ---------------------------- 105.6/434.1 MB 8.1 MB/s eta 0:00:41\n",
      "     --------- ---------------------------- 107.5/434.1 MB 8.1 MB/s eta 0:00:41\n",
      "     --------- ---------------------------- 108.8/434.1 MB 8.1 MB/s eta 0:00:41\n",
      "     --------- ---------------------------- 109.8/434.1 MB 8.1 MB/s eta 0:00:41\n",
      "     --------- ---------------------------- 110.9/434.1 MB 8.0 MB/s eta 0:00:41\n",
      "     --------- ---------------------------- 112.2/434.1 MB 8.0 MB/s eta 0:00:41\n",
      "     --------- ---------------------------- 113.2/434.1 MB 7.9 MB/s eta 0:00:41\n",
      "     ---------- --------------------------- 114.6/434.1 MB 7.9 MB/s eta 0:00:41\n",
      "     ---------- --------------------------- 115.6/434.1 MB 7.9 MB/s eta 0:00:41\n",
      "     ---------- --------------------------- 117.2/434.1 MB 7.8 MB/s eta 0:00:41\n",
      "     ---------- --------------------------- 119.3/434.1 MB 7.9 MB/s eta 0:00:41\n",
      "     ---------- --------------------------- 121.1/434.1 MB 7.9 MB/s eta 0:00:40\n",
      "     ---------- --------------------------- 123.2/434.1 MB 7.9 MB/s eta 0:00:40\n",
      "     ---------- --------------------------- 125.3/434.1 MB 7.9 MB/s eta 0:00:40\n",
      "     ----------- -------------------------- 127.1/434.1 MB 7.9 MB/s eta 0:00:39\n",
      "     ----------- -------------------------- 129.2/434.1 MB 7.9 MB/s eta 0:00:39\n",
      "     ----------- -------------------------- 131.1/434.1 MB 8.0 MB/s eta 0:00:39\n",
      "     ----------- -------------------------- 132.9/434.1 MB 8.0 MB/s eta 0:00:38\n",
      "     ----------- -------------------------- 135.0/434.1 MB 8.0 MB/s eta 0:00:38\n",
      "     ------------ ------------------------- 137.1/434.1 MB 8.0 MB/s eta 0:00:38\n",
      "     ------------ ------------------------- 139.2/434.1 MB 8.0 MB/s eta 0:00:37\n",
      "     ------------ ------------------------- 141.0/434.1 MB 8.0 MB/s eta 0:00:37\n",
      "     ------------ ------------------------- 142.9/434.1 MB 8.0 MB/s eta 0:00:37\n",
      "     ------------ ------------------------- 144.2/434.1 MB 8.0 MB/s eta 0:00:37\n",
      "     ------------ ------------------------- 146.0/434.1 MB 8.0 MB/s eta 0:00:36\n",
      "     ------------ ------------------------- 148.1/434.1 MB 8.0 MB/s eta 0:00:36\n",
      "     ------------- ------------------------ 149.7/434.1 MB 8.0 MB/s eta 0:00:36\n",
      "     ------------- ------------------------ 151.3/434.1 MB 8.0 MB/s eta 0:00:36\n",
      "     ------------- ------------------------ 153.4/434.1 MB 8.0 MB/s eta 0:00:35\n",
      "     ------------- ------------------------ 155.2/434.1 MB 8.0 MB/s eta 0:00:35\n",
      "     ------------- ------------------------ 157.3/434.1 MB 8.1 MB/s eta 0:00:35\n",
      "     ------------- ------------------------ 159.1/434.1 MB 8.1 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 161.2/434.1 MB 8.1 MB/s eta 0:00:34\n",
      "     -------------- ----------------------- 163.3/434.1 MB 8.1 MB/s eta 0:00:34\n",
      "     -------------- ----------------------- 165.2/434.1 MB 8.1 MB/s eta 0:00:34\n",
      "     -------------- ----------------------- 167.2/434.1 MB 8.1 MB/s eta 0:00:33\n",
      "     -------------- ----------------------- 169.1/434.1 MB 8.1 MB/s eta 0:00:33\n",
      "     -------------- ----------------------- 171.2/434.1 MB 8.1 MB/s eta 0:00:33\n",
      "     --------------- ---------------------- 173.0/434.1 MB 8.1 MB/s eta 0:00:33\n",
      "     --------------- ---------------------- 174.9/434.1 MB 8.1 MB/s eta 0:00:32\n",
      "     --------------- ---------------------- 176.9/434.1 MB 8.2 MB/s eta 0:00:32\n",
      "     --------------- ---------------------- 178.8/434.1 MB 8.2 MB/s eta 0:00:32\n",
      "     --------------- ---------------------- 180.9/434.1 MB 8.2 MB/s eta 0:00:31\n",
      "     --------------- ---------------------- 182.7/434.1 MB 8.2 MB/s eta 0:00:31\n",
      "     ---------------- --------------------- 184.8/434.1 MB 8.2 MB/s eta 0:00:31\n",
      "     ---------------- --------------------- 186.4/434.1 MB 8.2 MB/s eta 0:00:31\n",
      "     ---------------- --------------------- 188.2/434.1 MB 8.2 MB/s eta 0:00:31\n",
      "     ---------------- --------------------- 190.3/434.1 MB 8.2 MB/s eta 0:00:30\n",
      "     ---------------- --------------------- 191.9/434.1 MB 8.2 MB/s eta 0:00:30\n",
      "     ---------------- --------------------- 193.5/434.1 MB 8.2 MB/s eta 0:00:30\n",
      "     ----------------- -------------------- 195.3/434.1 MB 8.2 MB/s eta 0:00:30\n",
      "     ----------------- -------------------- 197.4/434.1 MB 8.2 MB/s eta 0:00:29\n",
      "     ----------------- -------------------- 199.5/434.1 MB 8.2 MB/s eta 0:00:29\n",
      "     ----------------- -------------------- 201.3/434.1 MB 8.2 MB/s eta 0:00:29\n",
      "     ----------------- -------------------- 203.4/434.1 MB 8.2 MB/s eta 0:00:29\n",
      "     ----------------- -------------------- 205.3/434.1 MB 8.2 MB/s eta 0:00:28\n",
      "     ------------------ ------------------- 207.4/434.1 MB 8.2 MB/s eta 0:00:28\n",
      "     ------------------ ------------------- 209.2/434.1 MB 8.3 MB/s eta 0:00:28\n",
      "     ------------------ ------------------- 211.3/434.1 MB 8.3 MB/s eta 0:00:27\n",
      "     ------------------ ------------------- 213.1/434.1 MB 8.3 MB/s eta 0:00:27\n",
      "     ------------------ ------------------- 215.2/434.1 MB 8.3 MB/s eta 0:00:27\n",
      "     ------------------ ------------------- 217.1/434.1 MB 8.3 MB/s eta 0:00:27\n",
      "     ------------------- ------------------ 218.6/434.1 MB 8.3 MB/s eta 0:00:27\n",
      "     ------------------- ------------------ 220.2/434.1 MB 8.3 MB/s eta 0:00:26\n",
      "     ------------------- ------------------ 222.0/434.1 MB 8.3 MB/s eta 0:00:26\n",
      "     ------------------- ------------------ 223.9/434.1 MB 8.3 MB/s eta 0:00:26\n",
      "     ------------------- ------------------ 226.0/434.1 MB 8.3 MB/s eta 0:00:26\n",
      "     ------------------- ------------------ 227.8/434.1 MB 8.3 MB/s eta 0:00:25\n",
      "     -------------------- ----------------- 229.9/434.1 MB 8.3 MB/s eta 0:00:25\n",
      "     -------------------- ----------------- 232.0/434.1 MB 8.3 MB/s eta 0:00:25\n",
      "     -------------------- ----------------- 233.8/434.1 MB 8.3 MB/s eta 0:00:25\n",
      "     -------------------- ----------------- 235.7/434.1 MB 8.3 MB/s eta 0:00:24\n",
      "     -------------------- ----------------- 237.8/434.1 MB 8.3 MB/s eta 0:00:24\n",
      "     -------------------- ----------------- 239.6/434.1 MB 8.3 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 241.7/434.1 MB 8.3 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 243.5/434.1 MB 8.3 MB/s eta 0:00:23\n",
      "     --------------------- ---------------- 245.6/434.1 MB 8.3 MB/s eta 0:00:23\n",
      "     --------------------- ---------------- 247.7/434.1 MB 8.3 MB/s eta 0:00:23\n",
      "     --------------------- ---------------- 249.6/434.1 MB 8.3 MB/s eta 0:00:23\n",
      "     ---------------------- --------------- 251.7/434.1 MB 8.4 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 253.5/434.1 MB 8.4 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 254.8/434.1 MB 8.3 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 256.6/434.1 MB 8.3 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 258.5/434.1 MB 8.4 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 260.6/434.1 MB 8.4 MB/s eta 0:00:21\n",
      "     ---------------------- --------------- 262.4/434.1 MB 8.4 MB/s eta 0:00:21\n",
      "     ----------------------- -------------- 264.5/434.1 MB 8.4 MB/s eta 0:00:21\n",
      "     ----------------------- -------------- 266.6/434.1 MB 8.5 MB/s eta 0:00:20\n",
      "     ----------------------- -------------- 268.4/434.1 MB 8.5 MB/s eta 0:00:20\n",
      "     ----------------------- -------------- 270.5/434.1 MB 8.5 MB/s eta 0:00:20\n",
      "     ----------------------- -------------- 272.4/434.1 MB 8.5 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 274.5/434.1 MB 8.6 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 276.3/434.1 MB 8.6 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 278.4/434.1 MB 8.6 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 280.5/434.1 MB 8.6 MB/s eta 0:00:18\n",
      "     ------------------------ ------------- 282.3/434.1 MB 8.6 MB/s eta 0:00:18\n",
      "     ------------------------ ------------- 284.4/434.1 MB 8.6 MB/s eta 0:00:18\n",
      "     ------------------------- ------------ 286.5/434.1 MB 8.6 MB/s eta 0:00:18\n",
      "     ------------------------- ------------ 288.4/434.1 MB 8.6 MB/s eta 0:00:18\n",
      "     ------------------------- ------------ 290.5/434.1 MB 8.6 MB/s eta 0:00:17\n",
      "     ------------------------- ------------ 292.0/434.1 MB 8.6 MB/s eta 0:00:17\n",
      "     ------------------------- ------------ 293.9/434.1 MB 8.6 MB/s eta 0:00:17\n",
      "     ------------------------- ------------ 296.0/434.1 MB 8.6 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 297.8/434.1 MB 8.6 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 299.9/434.1 MB 8.6 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 301.7/434.1 MB 8.6 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 303.8/434.1 MB 8.6 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 305.7/434.1 MB 8.6 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 307.8/434.1 MB 8.6 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 309.6/434.1 MB 8.6 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 311.7/434.1 MB 8.6 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 313.8/434.1 MB 8.6 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 315.6/434.1 MB 8.6 MB/s eta 0:00:14\n",
      "     --------------------------- ---------- 317.5/434.1 MB 8.6 MB/s eta 0:00:14\n",
      "     --------------------------- ---------- 319.6/434.1 MB 8.6 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 321.7/434.1 MB 8.7 MB/s eta 0:00:13\n",
      "     ---------------------------- --------- 323.5/434.1 MB 8.7 MB/s eta 0:00:13\n",
      "     ---------------------------- --------- 325.6/434.1 MB 8.7 MB/s eta 0:00:13\n",
      "     ---------------------------- --------- 327.4/434.1 MB 8.7 MB/s eta 0:00:13\n",
      "     ---------------------------- --------- 329.0/434.1 MB 8.7 MB/s eta 0:00:13\n",
      "     ---------------------------- --------- 331.1/434.1 MB 8.7 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 332.9/434.1 MB 8.7 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 335.0/434.1 MB 8.7 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 337.1/434.1 MB 8.7 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 339.0/434.1 MB 8.7 MB/s eta 0:00:11\n",
      "     ----------------------------- -------- 340.8/434.1 MB 8.7 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 342.9/434.1 MB 8.7 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 344.7/434.1 MB 8.7 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 346.8/434.1 MB 8.7 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 348.9/434.1 MB 8.7 MB/s eta 0:00:10\n",
      "     ------------------------------ ------- 350.7/434.1 MB 8.7 MB/s eta 0:00:10\n",
      "     ------------------------------ ------- 352.8/434.1 MB 8.7 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 354.7/434.1 MB 8.7 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 356.5/434.1 MB 8.7 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 358.1/434.1 MB 8.7 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 359.7/434.1 MB 8.7 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 361.5/434.1 MB 8.7 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 363.3/434.1 MB 8.7 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 365.2/434.1 MB 8.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ----- 367.3/434.1 MB 8.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ----- 369.1/434.1 MB 8.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ----- 371.2/434.1 MB 8.7 MB/s eta 0:00:08\n",
      "     -------------------------------- ----- 373.0/434.1 MB 8.8 MB/s eta 0:00:07\n",
      "     -------------------------------- ----- 375.1/434.1 MB 8.8 MB/s eta 0:00:07\n",
      "     -------------------------------- ----- 377.0/434.1 MB 8.9 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 379.1/434.1 MB 8.9 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 380.9/434.1 MB 8.9 MB/s eta 0:00:06\n",
      "     --------------------------------- ---- 383.0/434.1 MB 8.9 MB/s eta 0:00:06\n",
      "     --------------------------------- ---- 384.8/434.1 MB 8.9 MB/s eta 0:00:06\n",
      "     --------------------------------- ---- 386.9/434.1 MB 8.9 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 389.0/434.1 MB 8.9 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 390.9/434.1 MB 8.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 393.0/434.1 MB 8.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 394.8/434.1 MB 8.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 396.9/434.1 MB 8.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 398.7/434.1 MB 8.9 MB/s eta 0:00:04\n",
      "     ---------------------------------- --- 399.8/434.1 MB 8.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 401.6/434.1 MB 8.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 403.7/434.1 MB 8.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 405.5/434.1 MB 8.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 407.4/434.1 MB 8.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 409.2/434.1 MB 8.9 MB/s eta 0:00:03\n",
      "     ----------------------------------- -- 411.0/434.1 MB 8.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 412.9/434.1 MB 8.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 414.4/434.1 MB 8.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 416.8/434.1 MB 8.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 418.6/434.1 MB 8.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 420.7/434.1 MB 8.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 422.6/434.1 MB 8.9 MB/s eta 0:00:02\n",
      "     -------------------------------------  424.7/434.1 MB 8.9 MB/s eta 0:00:02\n",
      "     -------------------------------------  426.5/434.1 MB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  428.6/434.1 MB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  430.4/434.1 MB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  432.0/434.1 MB 8.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  433.3/434.1 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  434.1/434.1 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  434.1/434.1 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  434.1/434.1 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  434.1/434.1 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  434.1/434.1 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------  434.1/434.1 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- 434.1/434.1 MB 8.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.9 (from pyspark)\n",
      "  Downloading py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-4.0.0-py2.py3-none-any.whl size=434741273 sha256=4968841ba9125757475343c1adc697f93ba3ae8e7375cb5980fcbbffd6dc8bdd\n",
      "  Stored in directory: c:\\users\\sozha\\appdata\\local\\pip\\cache\\wheels\\2d\\77\\9b\\12660be70f7f447940a0caede37ae208b2e0d1c8487dce52a6\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.0.0\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8608f213-2920-4548-bdde-de9eb8069641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398aed2d-f367-42cd-bd4d-2f1e993f6c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krish</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sudhanshu</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harsha</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shubham</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name  age  Experience  Salary\n",
       "0      Krish   31          10   30000\n",
       "1  Sudhanshu   30           8   25000\n",
       "2      Sunny   29           4   20000\n",
       "3       Paul   24           3   20000\n",
       "4     Harsha   21           1   15000\n",
       "5    Shubham   23           2   18000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b77b7d-21fe-45ce-a57f-ee725b518530",
   "metadata": {},
   "source": [
    "# Creating spark session\n",
    "- To work with pyspark, we need to first create a spark session.\n",
    "- Lets create a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b9b4f86-4d9a-4979-ac9a-e1a502b2fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cb065e0-c9a2-4fb4-afc0-a2d8fd2d756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practice').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e242ae0c-6072-468f-bf42-369814c86b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Mini-T-800:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x13807cbaff0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4761ca8-4294-457e-ad20-36f393baedf6",
   "metadata": {},
   "source": [
    "# Reading data:\n",
    "- Now that we have the spark object, we can read data using it.\n",
    "- We have read object which has different ways to read data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38201cce-bfc3-44c1-9f64-7dfb05ef3250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark = spark.read.csv('test1.csv')\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc88682b-290a-41b7-a794-c16b65e4caa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|      _c0|_c1|       _c2|   _c3|\n",
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7867dcf-62e5-4334-b9f5-6a539cddc4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using header row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6ed17d6-fa45-4575-a8ba-cf7d2aee148b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: string, Experience: string, Salary: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark = spark.read.option('header',True).csv('test1.csv')\n",
    "df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09222369-e2ad-4cc2-9d91-5b3fd3094d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767d13b-368c-43fa-b587-80db7a5d3892",
   "metadata": {},
   "source": [
    "# lets look at the type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d336d6-9a3b-4edd-bac2-803285aadc96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.classic.dataframe.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf5fc1d-8e9e-4c3d-b9b0-cc8fe8919a1d",
   "metadata": {},
   "source": [
    "# head function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "666681b2-c6ee-48b6-9f7b-63fc26aac9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', age='31', Experience='10', Salary='30000'),\n",
       " Row(Name='Sudhanshu', age='30', Experience='8', Salary='25000'),\n",
       " Row(Name='Sunny', age='29', Experience='4', Salary='20000'),\n",
       " Row(Name='Paul', age='24', Experience='3', Salary='20000'),\n",
       " Row(Name='Harsha', age='21', Experience='1', Salary='15000')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.head(5) # note: we need to provide the number of rows we want it to display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f324d310-9626-4c6b-a84e-9cfd77d78d4c",
   "metadata": {},
   "source": [
    "# schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "398573db-bff4-4bcc-9add-c05bca1417d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f6fb7-3682-4d18-816f-dda23a9f2897",
   "metadata": {},
   "source": [
    "- PySpark Dataframe\n",
    "- Reading The Dataset\n",
    "- Checking the Datatypes of the Column(Schema)\n",
    "- Selecting Columns And Indexing\n",
    "- Check Describe option similar to Pandas\n",
    "- Adding Columns\n",
    "- Dropping Columns\n",
    "- Renaming column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12db698e-1579-43ef-8a0e-b1b2e48dc1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('test1.csv', inferSchema = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94246624-065a-4dd0-8693-5b30d4211f80",
   "metadata": {},
   "source": [
    "# note inferSchema will detect the datatypes automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37437dff-5f8b-4cca-8904-751acedb24ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "421a0d52-1b9f-43b6-a926-fc3cc1a4136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a3609ddf-9a29-4fbf-bca8-33cd4ae38846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do the same in the following way\n",
    "\n",
    "df_pyspark = spark.read.csv('test1.csv', inferSchema = True, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c9a7c9d-f345-4071-8db5-3d7cb62094df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1ade964-3191-4f50-84a5-3bd6037148e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4205813c-86c3-4620-add6-89c13de0ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working with columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b12499b-8da7-4433-945a-d27922ff9db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbdf5152-9344-4605-8649-fb58e5a9aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff3eb1a5-102c-4350-91dd-3c58f2a3e661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f9f2f73b-336f-433e-a15e-a7ffb4e20cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|     Paul|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1411cfb-b355-485b-862f-f036f1d5ceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, Experience: int]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select(['name', 'Experience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c06e222-7458-46b2-85d6-a8d7c320fa28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     name|Experience|\n",
      "+---------+----------+\n",
      "|    Krish|        10|\n",
      "|Sudhanshu|         8|\n",
      "|    Sunny|         4|\n",
      "|     Paul|         3|\n",
      "|   Harsha|         1|\n",
      "|  Shubham|         2|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['name', 'Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3e8a412d-3096-4c01-b674-77c69aae93f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name'] # this only tells us that it is a column. we will not be able to get the data using this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c876c7bd-5da7-4460-989b-952ac9618bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70a759b1-115a-484b-97a3-8d962ffa3214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "529bfa50-3f64-4b4a-9de5-b57ce27a7994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ea14f87-c7a6-4492-8e99-f743e3dff48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+------------------+\n",
      "|summary|  Name|               age|       Experience|            Salary|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "|  count|     6|                 6|                6|                 6|\n",
      "|   mean|  NULL|26.333333333333332|4.666666666666667|21333.333333333332|\n",
      "| stddev|  NULL| 4.179314138308661|3.559026084010437| 5354.126134736337|\n",
      "|    min|Harsha|                21|                1|             15000|\n",
      "|    max| Sunny|                31|               10|             30000|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfc78231-b717-4c95-baac-8d477c18443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f588d983-83f8-4329-af26-399afecb19c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience'] + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "26c238e0-c199-408e-aa0b-f4187b06fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+------------------------+\n",
      "|     Name|age|Experience|Salary|Experience After 2 Years|\n",
      "+---------+---+----------+------+------------------------+\n",
      "|    Krish| 31|        10| 30000|                      12|\n",
      "|Sudhanshu| 30|         8| 25000|                      10|\n",
      "|    Sunny| 29|         4| 20000|                       6|\n",
      "|     Paul| 24|         3| 20000|                       5|\n",
      "|   Harsha| 21|         1| 15000|                       3|\n",
      "|  Shubham| 23|         2| 18000|                       4|\n",
      "+---------+---+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d1c7edc4-2b33-4903-aa55-ce3d04606bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dfa52eed-b895-4f7e-b000-a640cfd5b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop('Experience After 2 Years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a41b5542-015a-43e8-9146-436bce0a10ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2808ec00-7bab-459b-985a-7fc04b68e5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column\n",
    "new_df = new_df.withColumnRenamed('name', 'New Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e3f90d8-2424-4dd4-a2d8-ff44cec9e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "| New Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706f8b8-9e16-46ea-ab82-6496d1e2a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df_pyspark.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b13a0be-ade7-448d-acac-4c7d48592e0d",
   "metadata": {},
   "source": [
    "# Pyspark Mandling missing values\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter in Dropping functionalities\n",
    "- Handling Missing values by Mean, Median and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0aa0eecc-b74f-4ea8-9551-32acef8d14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dcd425e-e684-461c-b43a-193bd4f4cf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "613e1829-cc92-404c-8163-1e291b977a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ed661f8c-1d90-4880-9b7f-6f7e698d2152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|NULL|      NULL| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      NULL|  NULL|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dropping the columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2312c07-a115-4c3a-9351-6c150873fa84",
   "metadata": {},
   "source": [
    " - drop all nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d4ee692-1d85-4e7a-883c-8d7a29dd440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c2f332-d719-488f-a10f-564c68e68ae7",
   "metadata": {},
   "source": [
    "- how how = any/all parameter\n",
    "- when using how = 'all' , the data will be deleted only when all the columns are nulls.\n",
    "- the default value for the how is any. it will delete rows if there is any nulls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7fc000d-220f-4eda-bb94-3c593215ec0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(how = 'all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1860f3d7-5d76-4218-8fd9-08781f3c87f1",
   "metadata": {},
   "source": [
    "- threshold thresh = 2\n",
    "- It will keep all the rows with the number or greater number of non-null values as the threshold.\n",
    "- In the following Example, it will keep all the roww with a minimum of 2 non null values in the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "211e1fcf-1444-4b30-915f-e2c07e14ef3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(thresh = 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e95702-caeb-45ab-a717-4af68dbe90a8",
   "metadata": {},
   "source": [
    "- in the following example, it keeps all the rows with atleast 1 non null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7d0c4c3b-757b-4437-8f99-0fe601b00006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(thresh = 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e224e1-1701-446c-9ed2-a333a4746f2d",
   "metadata": {},
   "source": [
    "- subset subset = [col1, col2...]\n",
    "- it tells if we want to check the null values only in specific columns.\n",
    "- In the following example, we are removing nulls available only in Experience column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5325738-d861-4f16-97b6-010f23ea8eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop(subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd26ca13-a3df-4084-8867-c442f1b0c8c0",
   "metadata": {},
   "source": [
    "# filling missing values\n",
    "- value\n",
    "- subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881614f4-d938-47c5-bd19-f0ce5676961e",
   "metadata": {},
   "source": [
    "- note - na.fill will only replace the null with the anoter value depending on the value provided.\n",
    "- if we provide 'Missing Values', then it will only replace in string.\n",
    "- if we provide -1, then it will repace in numbers columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e64d903c-044f-40a2-bc41-626727ef065a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|     Sudhanshu|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|NULL|      NULL| 40000|\n",
      "|Missing Values|  34|        10| 38000|\n",
      "|Missing Values|  36|      NULL|  NULL|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5040dae-5627-488e-8a9f-77760443d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh| -1|        -1| 40000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "|     NULL| 36|        -1|    -1|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(-1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d3b443-e531-475b-a17d-420b860d9702",
   "metadata": {},
   "source": [
    "- if we need to replace in multiple columns, we can provide a dictionary with different column names and its replacement value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca373dbf-6cfb-4fec-bbb6-74f24e42524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---+----------+------+\n",
      "|         Name|age|Experience|Salary|\n",
      "+-------------+---+----------+------+\n",
      "|        Krish| 31|        10| 30000|\n",
      "|    Sudhanshu| 30|         8| 25000|\n",
      "|        Sunny| 29|         4| 20000|\n",
      "|         Paul| 24|         3| 20000|\n",
      "|       Harsha| 21|         1| 15000|\n",
      "|      Shubham| 23|         2| 18000|\n",
      "|       Mahesh| -1|        -1| 40000|\n",
      "|Missing Value| 34|        10| 38000|\n",
      "|Missing Value| 36|        -1|    -1|\n",
      "+-------------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill({\n",
    "    'Name': 'Missing Value',\n",
    "    'Experience' : -1,\n",
    "    'age': -1,\n",
    "    'Salary': -1\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e0dce2d-efa5-4f64-9595-91cda0919ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eaf70d-cc74-4ed2-9558-20355fd66d65",
   "metadata": {},
   "source": [
    "- Replacing with mean using imputer function\n",
    "- pyspark comes with an Imputer class which helps to impute data.\n",
    "- it has inputCols and outputCols\n",
    "- the setStragegy takes the the type of imputation that we want to perform. (mean/median/mode)\n",
    "- we can then use the imputer object to perform fit and then transform the data.\n",
    "- As it is an ml model, we need to fit the data and then transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c2b7bd2-2deb-42ff-bb8d-b02772bc1b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "inputCols = ['age', 'Experience', 'Salary']\n",
    "outputCols = [f'{col}_imputed' for col in inputCols]\n",
    "imputer = Imputer(\n",
    "    inputCols = inputCols,\n",
    "    outputCols = outputCols,\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4995d71f-2bff-43ab-9e6a-e08fec7c6a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|         28|                 5|         40000|\n",
      "|     NULL|  34|        10| 38000|         34|                10|         38000|\n",
      "|     NULL|  36|      NULL|  NULL|         36|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "30c2d0a6-cf2f-48f6-81e4-47d33aeb844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "200640e8-add2-4ab0-aa3c-be1228222e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "inputCols = ['age', 'Experience', 'Salary']\n",
    "outputCols = [f'{col}_imputed' for col in inputCols]\n",
    "imputer = Imputer(\n",
    "    inputCols = inputCols,\n",
    "    outputCols = outputCols,\n",
    ").setStrategy('median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77b72c9a-5d0d-4687-97ae-106bb40e5b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|         29|                 4|         40000|\n",
      "|     NULL|  34|        10| 38000|         34|                10|         38000|\n",
      "|     NULL|  36|      NULL|  NULL|         36|                 4|         20000|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e57a64e-9230-4a5d-a5d9-45d473104288",
   "metadata": {},
   "source": [
    "# Pyspark Drataframe filters\n",
    "- Filter Operations\n",
    "- &, |, ==\n",
    "- ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8cc518c9-692a-4054-a27f-25456a96d764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "75446186-b986-4599-bf34-ebd20855bb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a28081f8-52a9-44a9-b30c-734ea553e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c6dc4e35-cd96-4c59-91db-cbeb9708f09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# salary of people less than or equal to 20000\n",
    "df_pyspark.filter('Salary <=20000').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b8812-8004-4492-b645-893ae25887cc",
   "metadata": {},
   "source": [
    "- selecting specific columns aftet filtering data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b2b808da-8939-40bc-8fc6-6771e2e851dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   Name|Salary|\n",
      "+-------+------+\n",
      "|  Sunny| 20000|\n",
      "|   Paul| 20000|\n",
      "| Harsha| 15000|\n",
      "|Shubham| 18000|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('Salary<=20000').select(['Name', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973b76e-e3c6-4008-a3eb-0dfe95b57b6e",
   "metadata": {},
   "source": [
    "- using masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "40a0f31b-f215-462e-b0b1-c78f260a3dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary'] <= 20000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45710661-75c6-4fd4-abcd-a25bd7e24f39",
   "metadata": {},
   "source": [
    "- multiple conditions\n",
    "- Note: we need a set of paranthesis for each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b5bfbb19-c27a-418c-9554-9e7f00c490dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary'] <= 20000) & \n",
    "                  (df_pyspark['Salary'] >=15000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a397429f-9d63-45ea-9ee9-06717fcf446b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Sunny| 29|         4| 20000|\n",
      "| Paul| 24|         3| 20000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Experience'] == 3) | \n",
    "                  (df_pyspark['Experience'] == 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0eb98f2e-13e3-4bc6-9b63-d7d9258b2674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary'] <= 20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7e93ff-c7d9-47e2-bb0b-a76b866a7c83",
   "metadata": {},
   "source": [
    "# Groupby and Aggregate functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "960141c2-5e87-47ac-8db0-e6edb800031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test3.csv', header = True, inferSchema = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "38c71fcb-b3ea-40a6-b278-1cf076d234e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "19e4cef6-acf2-4e39-8a72-e8c02a02b5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f64810-6f59-47fb-a686-87c25b5677e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "- groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05d76b7a-84fa-4f04-af06-d0faa6df2b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GroupedData[grouping expressions: [Name], value: [Name: string, Departments: string ... 1 more field], type: GroupBy]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupby('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b47ff8b-55fb-445b-b1d0-5dd3b1d886c3",
   "metadata": {},
   "source": [
    "- note: The groupBy() creates a GroupedData object and not a DataFrame. So, we cannot use a show() on it.\n",
    "- we need to apply aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9eaf94aa-8eb2-43d3-a9cf-6f8e41ad0f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, sum(salary): bigint]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6ece157a-f43f-4ec5-8ef5-4b6d9d483869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ccf293a-1b34-428d-af31-e8af91dfbf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Departments').avg().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fcafd714-abdf-4416-90f3-b4832ebec4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Departments').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b201ca66-a4ab-44dd-9f3f-c61906f7ba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      20000|\n",
      "|    Sunny|      10000|\n",
      "|    Krish|      10000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupby('Name').max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6370cf7-b5f3-43cc-8081-d23a46cdbeb4",
   "metadata": {},
   "source": [
    "# using agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "013c2b99-8653-400d-9c11-3371a1333c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "319c8b85-0704-4699-b29b-8c39100d8d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| Name|Salary|\n",
      "+-----+------+\n",
      "|Sunny|  2000|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# getting name of the person with the minimum salary.\n",
    "# Step 1: Get the minimum salary value\n",
    "min_salary = df_pyspark.agg({'Salary': 'min'}).collect()[0][0]\n",
    "min_salary\n",
    "# Step 2: Filter rows where Salary == min_salary\n",
    "df_pyspark.filter(df_pyspark[\"Salary\"] == min_salary).select(['Name', 'Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e76ab9-decc-4966-bf54-932a70324910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
