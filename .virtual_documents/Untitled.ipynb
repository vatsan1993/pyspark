!conda env list


!conda activate pyspark_env



!pip install pyspark


import pyspark


import pandas as pd
df = pd.read_csv('test1.csv')
df





from pyspark.sql import SparkSession


spark = SparkSession.builder.appName('Practice').getOrCreate()


spark





df_spark = spark.read.csv('test1.csv')
df_spark


df_spark.show()


# using header row.


df_spark = spark.read.option('header',True).csv('test1.csv')
df_spark


df_spark.show()





type(df_spark)





df_spark.head(5) # note: we need to provide the number of rows we want it to display





df_spark.printSchema()





df_pyspark = spark.read.option('header', 'true').csv('test1.csv', inferSchema = True)





df_pyspark


df_pyspark.printSchema()


# we can do the same in the following way

df_pyspark = spark.read.csv('test1.csv', inferSchema = True, header = True)


df_pyspark.printSchema()


df_pyspark.show()


# working with columns


df_pyspark.columns


# getting a column


df_pyspark.select('name')


df_pyspark.select('name').show()


df_pyspark.select(['name', 'Experience'])


df_pyspark.select(['name', 'Experience']).show()


df_pyspark['Name'] # this only tells us that it is a column. we will not be able to get the data using this.


#dtypes


df_pyspark.dtypes


# describe function


df_pyspark.describe().show()


# adding columns


new_df = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience'] + 2)


new_df.show()


# Dropping column


new_df = new_df.drop('Experience After 2 Years')


new_df.show()


# Renaming column
new_df = new_df.withColumnRenamed('name', 'New Name')


new_df.show()



