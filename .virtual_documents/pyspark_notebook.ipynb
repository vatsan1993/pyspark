!conda env list


!conda activate pyspark_env



!pip install pyspark


import pyspark


import pandas as pd
df = pd.read_csv('test1.csv')
df





from pyspark.sql import SparkSession


spark = SparkSession.builder.appName('Practice').getOrCreate()


spark





df_spark = spark.read.csv('test1.csv')
df_spark


df_spark.show()


# using header row.


df_spark = spark.read.option('header',True).csv('test1.csv')
df_spark


df_spark.show()





type(df_spark)





df_spark.head(5) # note: we need to provide the number of rows we want it to display





df_spark.printSchema()





df_pyspark = spark.read.option('header', 'true').csv('test1.csv', inferSchema = True)





df_pyspark


df_pyspark.printSchema()


# we can do the same in the following way

df_pyspark = spark.read.csv('test1.csv', inferSchema = True, header = True)


df_pyspark.printSchema()


df_pyspark.show()


# working with columns


df_pyspark.columns


# getting a column


df_pyspark.select('name')


df_pyspark.select('name').show()


df_pyspark.select(['name', 'Experience'])


df_pyspark.select(['name', 'Experience']).show()


df_pyspark['Name'] # this only tells us that it is a column. we will not be able to get the data using this.


#dtypes


df_pyspark.dtypes


# describe function


df_pyspark.describe().show()


# adding columns


new_df = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience'] + 2)


new_df.show()


# Dropping column


new_df = new_df.drop('Experience After 2 Years')


new_df.show()


# Renaming column
new_df = new_df.withColumnRenamed('name', 'New Name')


new_df.show()


new_df = df_pyspark.drop()





df_pyspark = spark.read.csv('test2.csv', header = True, inferSchema = True)


df_pyspark


df_pyspark.show()


# dropping the columns
df_pyspark.drop('Name').show()





df_pyspark.na.drop().show()





df_pyspark.na.drop(how = 'all').show()





df_pyspark.na.drop(thresh = 2).show()





df_pyspark.na.drop(thresh = 1).show()





df_pyspark.na.drop(subset=['Experience']).show()








df_pyspark.na.fill('Missing Values').show()


df_pyspark.na.fill(-1).show()





df_pyspark.na.fill({
    'Name': 'Missing Value',
    'Experience' : -1,
    'age': -1,
    'Salary': -1
}).show()


df_pyspark.printSchema()






from pyspark.ml.feature import Imputer

inputCols = ['age', 'Experience', 'Salary']
outputCols = [f'{col}_imputed' for col in inputCols]
imputer = Imputer(
    inputCols = inputCols,
    outputCols = outputCols,
).setStrategy('mean')


imputer.fit(df_pyspark).transform(df_pyspark).show()


# replacing with median


from pyspark.ml.feature import Imputer

inputCols = ['age', 'Experience', 'Salary']
outputCols = [f'{col}_imputed' for col in inputCols]
imputer = Imputer(
    inputCols = inputCols,
    outputCols = outputCols,
).setStrategy('median')


imputer.fit(df_pyspark).transform(df_pyspark).show()





df_pyspark = spark.read.csv('test1.csv', header = True, inferSchema = True)


df_pyspark


df_pyspark.show()


# salary of people less than or equal to 20000
df_pyspark.filter('Salary <=20000').show()





df_pyspark.filter('Salary<=20000').select(['Name', 'Salary']).show()





df_pyspark.filter(df_pyspark['Salary'] <= 20000).show()





df_pyspark.filter((df_pyspark['Salary'] <= 20000) & 
                  (df_pyspark['Salary'] >=15000)).show()


df_pyspark.filter((df_pyspark['Experience'] == 3) | 
                  (df_pyspark['Experience'] == 4)).show()


df_pyspark.filter(~(df_pyspark['Salary'] <= 20000)).show()





df_pyspark = spark.read.csv('test3.csv', header = True, inferSchema = True)



df_pyspark.show()


df_pyspark.printSchema()


- groupby


df_pyspark.groupby('Name')





df_pyspark.groupby('Name').sum()


df_pyspark.groupby('Name').sum().show()


df_pyspark.groupby('Departments').avg().show()


df_pyspark.groupby('Departments').count().show()


df_pyspark.groupby('Name').max().show()





df_pyspark.agg({'Salary': 'sum'}).show()


df_pyspark.agg({'Salary': 'min'}).show()



